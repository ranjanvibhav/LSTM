{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaniports = pd.read_csv('archive/adaniports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of             Date      Symbol Series  Prev Close    Open     High     Low  \\\n",
       "0     2007-11-27  MUNDRAPORT     EQ      440.00  770.00  1050.00  770.00   \n",
       "1     2007-11-28  MUNDRAPORT     EQ      962.90  984.00   990.00  874.00   \n",
       "2     2007-11-29  MUNDRAPORT     EQ      893.90  909.00   914.75  841.00   \n",
       "3     2007-11-30  MUNDRAPORT     EQ      884.20  890.00   958.00  890.00   \n",
       "4     2007-12-03  MUNDRAPORT     EQ      921.55  939.75   995.00  922.00   \n",
       "...          ...         ...    ...         ...     ...      ...     ...   \n",
       "3317  2021-04-26  ADANIPORTS     EQ      725.35  733.00   739.65  728.90   \n",
       "3318  2021-04-27  ADANIPORTS     EQ      730.75  735.00   757.50  727.35   \n",
       "3319  2021-04-28  ADANIPORTS     EQ      749.15  755.00   760.00  741.10   \n",
       "3320  2021-04-29  ADANIPORTS     EQ      746.25  753.20   765.85  743.40   \n",
       "3321  2021-04-30  ADANIPORTS     EQ      746.75  739.00   759.45  724.50   \n",
       "\n",
       "       Last   Close    VWAP    Volume      Turnover    Trades  \\\n",
       "0     959.0  962.90  984.72  27294366  2.687719e+15       NaN   \n",
       "1     885.0  893.90  941.38   4581338  4.312765e+14       NaN   \n",
       "2     887.0  884.20  888.09   5124121  4.550658e+14       NaN   \n",
       "3     929.0  921.55  929.17   4609762  4.283257e+14       NaN   \n",
       "4     980.0  969.30  965.65   2977470  2.875200e+14       NaN   \n",
       "...     ...     ...     ...       ...           ...       ...   \n",
       "3317  729.2  730.75  733.25   9390549  6.885658e+14  116457.0   \n",
       "3318  748.6  749.15  747.67  20573107  1.538191e+15  236896.0   \n",
       "3319  743.4  746.25  751.02  11156977  8.379106e+14  130847.0   \n",
       "3320  746.4  746.75  753.06  13851910  1.043139e+15  153293.0   \n",
       "3321  726.4  730.05  743.35  12600934  9.366911e+14  132141.0   \n",
       "\n",
       "      Deliverable Volume  %Deliverble  \n",
       "0                9859619       0.3612  \n",
       "1                1453278       0.3172  \n",
       "2                1069678       0.2088  \n",
       "3                1260913       0.2735  \n",
       "4                 816123       0.2741  \n",
       "...                  ...          ...  \n",
       "3317              838079       0.0892  \n",
       "3318             1779639       0.0865  \n",
       "3319             1342353       0.1203  \n",
       "3320             1304895       0.0942  \n",
       "3321             3514692       0.2789  \n",
       "\n",
       "[3322 rows x 15 columns]>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaniports.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Symbol', 'Series', 'Prev Close', 'Open', 'High', 'Low', 'Last', 'Close', 'VWAP', 'Volume', 'Turnover', 'Trades', 'Deliverable Volume', '%Deliverble']\n"
     ]
    }
   ],
   "source": [
    "print(list(adaniports))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adaniports = adaniports.drop(['Symbol','Series', 'Turnover','Trades'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaniports['Date'] = pd.to_datetime(adaniports['Date'], format = '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "columns = ['Prev Close', 'Open', 'High','Low', 'Last','Close', 'VWAP','Volume','Deliverable Volume','%Deliverble']\n",
    "adaniports[columns] = scaler.fit_transform(adaniports[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Prev Close</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Last</th>\n",
       "      <th>Close</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Deliverable Volume</th>\n",
       "      <th>%Deliverble</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2007-11-27</td>\n",
       "      <td>0.276794</td>\n",
       "      <td>0.550634</td>\n",
       "      <td>0.774216</td>\n",
       "      <td>0.570576</td>\n",
       "      <td>0.709167</td>\n",
       "      <td>0.712743</td>\n",
       "      <td>0.734103</td>\n",
       "      <td>0.279227</td>\n",
       "      <td>0.439703</td>\n",
       "      <td>0.322305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2007-11-28</td>\n",
       "      <td>0.712743</td>\n",
       "      <td>0.728634</td>\n",
       "      <td>0.724774</td>\n",
       "      <td>0.659896</td>\n",
       "      <td>0.647500</td>\n",
       "      <td>0.655217</td>\n",
       "      <td>0.697799</td>\n",
       "      <td>0.046763</td>\n",
       "      <td>0.064606</td>\n",
       "      <td>0.274102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2007-11-29</td>\n",
       "      <td>0.655217</td>\n",
       "      <td>0.666251</td>\n",
       "      <td>0.662766</td>\n",
       "      <td>0.631554</td>\n",
       "      <td>0.649167</td>\n",
       "      <td>0.647130</td>\n",
       "      <td>0.653161</td>\n",
       "      <td>0.052318</td>\n",
       "      <td>0.047490</td>\n",
       "      <td>0.155346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2007-11-30</td>\n",
       "      <td>0.647130</td>\n",
       "      <td>0.650447</td>\n",
       "      <td>0.698406</td>\n",
       "      <td>0.673638</td>\n",
       "      <td>0.684167</td>\n",
       "      <td>0.678269</td>\n",
       "      <td>0.687572</td>\n",
       "      <td>0.047054</td>\n",
       "      <td>0.056023</td>\n",
       "      <td>0.226227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2007-12-03</td>\n",
       "      <td>0.678269</td>\n",
       "      <td>0.691828</td>\n",
       "      <td>0.728895</td>\n",
       "      <td>0.701121</td>\n",
       "      <td>0.726667</td>\n",
       "      <td>0.718079</td>\n",
       "      <td>0.718129</td>\n",
       "      <td>0.030347</td>\n",
       "      <td>0.036176</td>\n",
       "      <td>0.226884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Prev Close      Open      High       Low      Last     Close  \\\n",
       "0 2007-11-27    0.276794  0.550634  0.774216  0.570576  0.709167  0.712743   \n",
       "1 2007-11-28    0.712743  0.728634  0.724774  0.659896  0.647500  0.655217   \n",
       "2 2007-11-29    0.655217  0.666251  0.662766  0.631554  0.649167  0.647130   \n",
       "3 2007-11-30    0.647130  0.650447  0.698406  0.673638  0.684167  0.678269   \n",
       "4 2007-12-03    0.678269  0.691828  0.728895  0.701121  0.726667  0.718079   \n",
       "\n",
       "       VWAP    Volume  Deliverable Volume  %Deliverble  \n",
       "0  0.734103  0.279227            0.439703     0.322305  \n",
       "1  0.697799  0.046763            0.064606     0.274102  \n",
       "2  0.653161  0.052318            0.047490     0.155346  \n",
       "3  0.687572  0.047054            0.056023     0.226227  \n",
       "4  0.718129  0.030347            0.036176     0.226884  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaniports.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prev Close</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Last</th>\n",
       "      <th>Close</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Deliverable Volume</th>\n",
       "      <th>%Deliverble</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.276794</td>\n",
       "      <td>0.550634</td>\n",
       "      <td>0.774216</td>\n",
       "      <td>0.570576</td>\n",
       "      <td>0.709167</td>\n",
       "      <td>0.712743</td>\n",
       "      <td>0.734103</td>\n",
       "      <td>0.279227</td>\n",
       "      <td>0.439703</td>\n",
       "      <td>0.322305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.712743</td>\n",
       "      <td>0.728634</td>\n",
       "      <td>0.724774</td>\n",
       "      <td>0.659896</td>\n",
       "      <td>0.647500</td>\n",
       "      <td>0.655217</td>\n",
       "      <td>0.697799</td>\n",
       "      <td>0.046763</td>\n",
       "      <td>0.064606</td>\n",
       "      <td>0.274102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.655217</td>\n",
       "      <td>0.666251</td>\n",
       "      <td>0.662766</td>\n",
       "      <td>0.631554</td>\n",
       "      <td>0.649167</td>\n",
       "      <td>0.647130</td>\n",
       "      <td>0.653161</td>\n",
       "      <td>0.052318</td>\n",
       "      <td>0.047490</td>\n",
       "      <td>0.155346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.647130</td>\n",
       "      <td>0.650447</td>\n",
       "      <td>0.698406</td>\n",
       "      <td>0.673638</td>\n",
       "      <td>0.684167</td>\n",
       "      <td>0.678269</td>\n",
       "      <td>0.687572</td>\n",
       "      <td>0.047054</td>\n",
       "      <td>0.056023</td>\n",
       "      <td>0.226227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.678269</td>\n",
       "      <td>0.691828</td>\n",
       "      <td>0.728895</td>\n",
       "      <td>0.701121</td>\n",
       "      <td>0.726667</td>\n",
       "      <td>0.718079</td>\n",
       "      <td>0.718129</td>\n",
       "      <td>0.030347</td>\n",
       "      <td>0.036176</td>\n",
       "      <td>0.226884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.501730</td>\n",
       "      <td>0.492410</td>\n",
       "      <td>0.538297</td>\n",
       "      <td>0.494353</td>\n",
       "      <td>0.517500</td>\n",
       "      <td>0.517529</td>\n",
       "      <td>0.524279</td>\n",
       "      <td>0.003487</td>\n",
       "      <td>0.004696</td>\n",
       "      <td>0.269829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.517529</td>\n",
       "      <td>0.521522</td>\n",
       "      <td>0.547608</td>\n",
       "      <td>0.540516</td>\n",
       "      <td>0.553333</td>\n",
       "      <td>0.553254</td>\n",
       "      <td>0.547817</td>\n",
       "      <td>0.003380</td>\n",
       "      <td>0.006636</td>\n",
       "      <td>0.419369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.553254</td>\n",
       "      <td>0.558120</td>\n",
       "      <td>0.557414</td>\n",
       "      <td>0.562159</td>\n",
       "      <td>0.549167</td>\n",
       "      <td>0.546834</td>\n",
       "      <td>0.556119</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>0.001881</td>\n",
       "      <td>0.305653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.546834</td>\n",
       "      <td>0.533999</td>\n",
       "      <td>0.535248</td>\n",
       "      <td>0.527633</td>\n",
       "      <td>0.526667</td>\n",
       "      <td>0.520322</td>\n",
       "      <td>0.521247</td>\n",
       "      <td>0.001895</td>\n",
       "      <td>0.003749</td>\n",
       "      <td>0.422546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.520322</td>\n",
       "      <td>0.543980</td>\n",
       "      <td>0.536896</td>\n",
       "      <td>0.536308</td>\n",
       "      <td>0.525750</td>\n",
       "      <td>0.524907</td>\n",
       "      <td>0.530394</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>0.257778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Prev Close      Open      High       Low      Last     Close      VWAP  \\\n",
       "0     0.276794  0.550634  0.774216  0.570576  0.709167  0.712743  0.734103   \n",
       "1     0.712743  0.728634  0.724774  0.659896  0.647500  0.655217  0.697799   \n",
       "2     0.655217  0.666251  0.662766  0.631554  0.649167  0.647130  0.653161   \n",
       "3     0.647130  0.650447  0.698406  0.673638  0.684167  0.678269  0.687572   \n",
       "4     0.678269  0.691828  0.728895  0.701121  0.726667  0.718079  0.718129   \n",
       "..         ...       ...       ...       ...       ...       ...       ...   \n",
       "56    0.501730  0.492410  0.538297  0.494353  0.517500  0.517529  0.524279   \n",
       "57    0.517529  0.521522  0.547608  0.540516  0.553333  0.553254  0.547817   \n",
       "58    0.553254  0.558120  0.557414  0.562159  0.549167  0.546834  0.556119   \n",
       "59    0.546834  0.533999  0.535248  0.527633  0.526667  0.520322  0.521247   \n",
       "60    0.520322  0.543980  0.536896  0.536308  0.525750  0.524907  0.530394   \n",
       "\n",
       "      Volume  Deliverable Volume  %Deliverble  \n",
       "0   0.279227            0.439703     0.322305  \n",
       "1   0.046763            0.064606     0.274102  \n",
       "2   0.052318            0.047490     0.155346  \n",
       "3   0.047054            0.056023     0.226227  \n",
       "4   0.030347            0.036176     0.226884  \n",
       "..       ...                 ...          ...  \n",
       "56  0.003487            0.004696     0.269829  \n",
       "57  0.003380            0.006636     0.419369  \n",
       "58  0.001280            0.001881     0.305653  \n",
       "59  0.001895            0.003749     0.422546  \n",
       "60  0.001159            0.001453     0.257778  \n",
       "\n",
       "[61 rows x 10 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaniports.loc[60-60:60,columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(60,3322):\n",
    "    X_train.append(adaniports.loc[i-60:i,columns].to_numpy())\n",
    "    y_train.append(adaniports.loc[i, 'Close'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3262, 61, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3262/3262 [==============================] - 11s 3ms/step - loss: 0.0057\n",
      "Epoch 2/100\n",
      "3262/3262 [==============================] - 9s 3ms/step - loss: 0.0022\n",
      "Epoch 3/100\n",
      "3262/3262 [==============================] - 9s 3ms/step - loss: 0.0018\n",
      "Epoch 4/100\n",
      "3262/3262 [==============================] - 8s 3ms/step - loss: 0.0021\n",
      "Epoch 5/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 0.0015\n",
      "Epoch 6/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 0.0016\n",
      "Epoch 7/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 0.0012\n",
      "Epoch 8/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 0.0013\n",
      "Epoch 9/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 0.0013\n",
      "Epoch 10/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 0.0012\n",
      "Epoch 11/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 0.0012\n",
      "Epoch 12/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 9.9130e-04\n",
      "Epoch 13/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 0.0010\n",
      "Epoch 14/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 9.6179e-04\n",
      "Epoch 15/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 8.3670e-04\n",
      "Epoch 16/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 8.0511e-04\n",
      "Epoch 17/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 8.2271e-04\n",
      "Epoch 18/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 8.8632e-04\n",
      "Epoch 19/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 6.8938e-04\n",
      "Epoch 20/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 6.4414e-04\n",
      "Epoch 21/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 6.3712e-04\n",
      "Epoch 22/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 6.0723e-04\n",
      "Epoch 23/100\n",
      "3262/3262 [==============================] - 9s 3ms/step - loss: 6.0250e-04\n",
      "Epoch 24/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 5.2709e-04\n",
      "Epoch 25/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 5.0476e-04\n",
      "Epoch 26/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 5.4020e-04\n",
      "Epoch 27/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 5.3410e-04\n",
      "Epoch 28/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 4.7426e-04\n",
      "Epoch 29/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 4.5973e-04\n",
      "Epoch 30/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 4.7495e-04\n",
      "Epoch 31/100\n",
      "3262/3262 [==============================] - 8s 3ms/step - loss: 4.3890e-04\n",
      "Epoch 32/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 4.3810e-04\n",
      "Epoch 33/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 3.9361e-04\n",
      "Epoch 34/100\n",
      "3262/3262 [==============================] - 8s 3ms/step - loss: 4.2986e-04\n",
      "Epoch 35/100\n",
      "3262/3262 [==============================] - 9s 3ms/step - loss: 4.5914e-04\n",
      "Epoch 36/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 4.3075e-04\n",
      "Epoch 37/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 4.0739e-04\n",
      "Epoch 38/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 3.7631e-04\n",
      "Epoch 39/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 3.5794e-04\n",
      "Epoch 40/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 3.4263e-04\n",
      "Epoch 41/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 3.3974e-04\n",
      "Epoch 42/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 3.7999e-04\n",
      "Epoch 43/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 3.9499e-04\n",
      "Epoch 44/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 3.8986e-04\n",
      "Epoch 45/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 3.0574e-04\n",
      "Epoch 46/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 3.4452e-04\n",
      "Epoch 47/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 3.1387e-04\n",
      "Epoch 48/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 3.4481e-04\n",
      "Epoch 49/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 3.4110e-04\n",
      "Epoch 50/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 3.6204e-04\n",
      "Epoch 51/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.9282e-04\n",
      "Epoch 52/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 3.2659e-04\n",
      "Epoch 53/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 3.0477e-04\n",
      "Epoch 54/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.6844e-04\n",
      "Epoch 55/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.6581e-04\n",
      "Epoch 56/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 3.1629e-04\n",
      "Epoch 57/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.8034e-04\n",
      "Epoch 58/100\n",
      "3262/3262 [==============================] - 9s 3ms/step - loss: 2.7297e-04\n",
      "Epoch 59/100\n",
      "3262/3262 [==============================] - 8s 3ms/step - loss: 2.8884e-04\n",
      "Epoch 60/100\n",
      "3262/3262 [==============================] - 9s 3ms/step - loss: 2.9422e-04\n",
      "Epoch 61/100\n",
      "3262/3262 [==============================] - 8s 3ms/step - loss: 2.7804e-04\n",
      "Epoch 62/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.7671e-04\n",
      "Epoch 63/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.8877e-04\n",
      "Epoch 64/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.6430e-04\n",
      "Epoch 65/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.6922e-04\n",
      "Epoch 66/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.3880e-04\n",
      "Epoch 67/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.7645e-04\n",
      "Epoch 68/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.5723e-04\n",
      "Epoch 69/100\n",
      "3262/3262 [==============================] - 10s 3ms/step - loss: 2.5521e-04\n",
      "Epoch 70/100\n",
      "3262/3262 [==============================] - 9s 3ms/step - loss: 2.5152e-04\n",
      "Epoch 71/100\n",
      "3262/3262 [==============================] - 9s 3ms/step - loss: 2.3606e-04\n",
      "Epoch 72/100\n",
      "3262/3262 [==============================] - 10s 3ms/step - loss: 2.5897e-04\n",
      "Epoch 73/100\n",
      "3262/3262 [==============================] - 9s 3ms/step - loss: 2.4036e-04\n",
      "Epoch 74/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.3830e-04\n",
      "Epoch 75/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.2627e-04\n",
      "Epoch 76/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.2384e-04\n",
      "Epoch 77/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.3216e-04\n",
      "Epoch 78/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.5208e-04\n",
      "Epoch 79/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.8174e-04\n",
      "Epoch 80/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.4626e-04\n",
      "Epoch 81/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.5497e-04\n",
      "Epoch 82/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.3392e-04\n",
      "Epoch 83/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.6872e-04\n",
      "Epoch 84/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.5767e-04\n",
      "Epoch 85/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.9553e-04\n",
      "Epoch 86/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.8740e-04\n",
      "Epoch 87/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.5875e-04\n",
      "Epoch 88/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.2105e-04\n",
      "Epoch 89/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.3128e-04\n",
      "Epoch 90/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.0620e-04\n",
      "Epoch 91/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.1031e-04\n",
      "Epoch 92/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.2936e-04\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.4037e-04\n",
      "Epoch 94/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.6301e-04\n",
      "Epoch 95/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.3305e-04\n",
      "Epoch 96/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.2686e-04\n",
      "Epoch 97/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.0200e-04\n",
      "Epoch 98/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 1.9260e-04\n",
      "Epoch 99/100\n",
      "3262/3262 [==============================] - 8s 2ms/step - loss: 2.1888e-04\n",
      "Epoch 100/100\n",
      "3262/3262 [==============================] - 9s 3ms/step - loss: 2.0034e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x17204524a88>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor = Sequential()\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 10)))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(Dense(units = 1))\n",
    "\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "regressor.fit(X_train, y_train, epochs = 100, batch_size = 61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.save('adani_lstm.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
